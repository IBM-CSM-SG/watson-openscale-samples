{"cells": [{"metadata": {"id": "130546d4-d72c-46f1-a506-246ace42ad56"}, "cell_type": "markdown", "source": "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {"id": "a6d32920-c6f8-4df6-91a7-925640a9920a"}, "cell_type": "markdown", "source": "# Working with a custom metrics provider"}, {"metadata": {"id": "ab7073b7-baf8-48ba-b9fb-e39e497596aa"}, "cell_type": "markdown", "source": "This notebook demonstrates how to configure the custom monitor and custom metrics deployment by using IBM Watson OpenScale. It requires service credentials for the following services:\n  * Watson OpenScale\n  * Watson Machine Learning\n\n  \n## Contents\n\nThis notebook contains the following parts:\n\n  1. [Set up your environment.](#setup)\n  1. [Create the custom metrics provider - python function.](#provider)\n  1. [Register the custom metrics provider and create a deployment.](#deployment)\n  1. [Configure Watson OpenScale](#config)\n  1. [Create the integrated system for the custom metrics provider.](#custom)\n  1. [Set up the custom monitor definition and instance.](#instance)\n"}, {"metadata": {"id": "8c3c7fcc-8701-45f3-ab34-af1b093b590c"}, "cell_type": "markdown", "source": "## 1. Set up your environment. <a name=\"setup\"></a>\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:"}, {"metadata": {"collapsed": true, "id": "8b4212ee-4116-4a47-a586-f0997df1ec53"}, "cell_type": "markdown", "source": "### Install the  `ibm-watson-machine-learning` and `ibm-watson-openscale` packages."}, {"metadata": {"id": "2eb75b61-93b5-4139-96d5-fae98c8c9173", "scrolled": true}, "cell_type": "code", "source": "!pip install --upgrade ibm-watson-machine-learning   | tail -n 1\n!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {"id": "cd55a125-14e4-440c-a2ee-c58813d75f9e"}, "cell_type": "markdown", "source": "### Action: restart the kernel!"}, {"metadata": {"id": "2abe81f0-cd47-4fe0-9b5c-505712c17752", "scrolled": true}, "cell_type": "markdown", "source": "### Credentials for IBM Cloud\nTo authenticate, in the following code boxes, replace the sample data with your own credentials. Get the information from your system administrator or through the IBM Cloud dashboard."}, {"metadata": {"id": "37f4abdea5d34ffd87dc6716b8ff989c"}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "############################################################################################\n# Paste your credentials into the following section and then run this cell.\n############################################################################################\nCLOUD_API_KEY = \"<Your Cloud IAM API Key>\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "IAM_URL=\"https://iam.ng.bluemix.net/oidc/token\"\nOPENSCALE_API_URL = \"https://api.aiopenscale.cloud.ibm.com\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "3f499be0-0ed7-46c9-9bf4-753c2529525f"}, "cell_type": "code", "source": "WML_CREDENTIALS = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": CLOUD_API_KEY\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "d1df5c3f-4532-405b-a14d-6018bbca41e2"}, "cell_type": "markdown", "source": "\n### Enter your Watson OpenScale GUID.\n\nFor most systems, the default GUID is already entered for you. You would only need to update this particular entry if the GUID was changed from the default.\n"}, {"metadata": {"id": "0a569d1f-2989-4f22-892f-3983f398ffce"}, "cell_type": "code", "source": "####################################################################\n# Paste your IBM Watson OpenScale DataMart Id in the following field and then run this cell.\n####################################################################\n\nWOS_GUID=\"<IBM Watson OpenScale DataMart Id>\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "9cce9ad8f9bf4fb095680d91ad38b2a0"}, "cell_type": "markdown", "source": "### Define the Watson OpenScale subscription to which the custom metrics have to be sent.\n\nCreate a subscription from the Watson Openscale UI or SDK to configure custom metrics. You can configure custom metrics for the subscriptions that have predefined monitors, such as fairness, quality, or drift or without predefined monitors."}, {"metadata": {"id": "48519343224a4ca582ee954c3a0e6605"}, "cell_type": "code", "source": "####################################################################\n# Paste your Subscription in the following field and then run this cell.\n####################################################################\n\nsubscription_id = \"<Subscription Id>\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "67584279ed094267bbcd98224d6d0b2f"}, "cell_type": "markdown", "source": "### Python function details"}, {"metadata": {"id": "cecf1dd131db4b378dd4217f998ef3f0"}, "cell_type": "code", "source": "PYTHON_FUNCTION_NAME = 'Custom Metrics Provider Function'\nDEPLOYMENT_NAME = 'Custom Metrics Provider Deployment'", "execution_count": null, "outputs": []}, {"metadata": {"id": "ee9bcd7ca4e9412881c93ecab30afdb5"}, "cell_type": "markdown", "source": "### OpenScale Custom Metrics Provider name"}, {"metadata": {"id": "7209da78bf914a0c942d2cbff79d72cb"}, "cell_type": "code", "source": "CUSTOM_METRICS_PROVIDER_NAME = \"Custom Metrics Provider\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "eebc2a4d71204bf1ac53ab1f5e92318c"}, "cell_type": "markdown", "source": "### OpenSale Custom Monitor name"}, {"metadata": {"id": "eea74c49408144aaa531949b018cdc5d"}, "cell_type": "code", "source": "###################################################################\n# UPDATE your custom monitor name in the following field and then run this cell.\n####################################################################\nCUSTOM_MONITOR_NAME = 'Sample Model Performance' ", "execution_count": null, "outputs": []}, {"metadata": {"id": "51dab6f2e41045f38ead783922814c0a"}, "cell_type": "markdown", "source": "## 2. Create the custom metrics provider - Python function. <a name=\"provider\"></a>\n\nThe Python function receives the required variables, such as the `datamart_id`, `monitor_instance_id`, `monitor_id`, `monitor_instance_parameters` and `subscription_id` from the Watson OpenScale service when it is invoked by the custom monitor. \n\nIn the Python function, add your own logic to compute the custom metrics in the `get_metrics` method, publish the metrics to the Watson Openscale service and update the status of the run to the `finished` state in the custom monitor instance.\n\nUpdate the `WOS_CREDENTIALS` in the Python function. "}, {"metadata": {"id": "e7ec6979ce114f528246ba99e8619090"}, "cell_type": "code", "source": "#wml_python_function\nparms = {\n        \"url\": OPENSCALE_API_URL,\n        \"iam_url\": IAM_URL,\n        \"apikey\": CLOUD_API_KEY\n    }\ndef custom_metrics_provider(parms = parms):\n    \n    import json\n    import requests\n    import base64\n    from requests.auth import HTTPBasicAuth\n    import time\n    import uuid    \n    \n    headers = {}\n    headers[\"Content-Type\"] = \"application/json\"\n    headers[\"Accept\"] = \"application/json\"\n\n    def get_access_token():\n        headers={}\n        headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n        headers[\"Accept\"] = \"application/json\"\n        auth = HTTPBasicAuth(\"bx\", \"bx\")\n        data = {\n            \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n            \"apikey\": parms[\"apikey\"]\n        }\n        response = requests.post(parms[\"iam_url\"], data=data, headers=headers, auth=auth)\n        json_data = response.json()\n        access_token = json_data['access_token']\n        return access_token    \n    \n    def get_feedback_dataset_id(access_token, data_mart_id, subscription_id):\n        headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n        DATASETS_URL =  parms[\"url\"] + \"/openscale/{0}/v2/data_sets?target.target_id={1}&target.target_type=subscription&type=feedback\".format(data_mart_id, subscription_id)\n        response = requests.get(DATASETS_URL, headers=headers, verify=False)\n        json_data = response.json()\n        feedback_dataset_id = None\n        if \"data_sets\" in json_data and len(json_data[\"data_sets\"]) > 0:\n            feedback_dataset_id = json_data[\"data_sets\"][0][\"metadata\"][\"id\"]\n        \n        return feedback_dataset_id\n    \n    def get_feedback_data(access_token, data_mart_id, feedback_dataset_id):\n        json_data = None\n        if feedback_dataset_id is not None:\n            headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n            DATASETS_STORE_RECORDS_URL = parms[\"url\"] + \"/openscale/{0}/v2/data_sets/{1}/records?limit={2}&format=list\".format(data_mart_id, feedback_dataset_id, 100)\n            response = requests.get(DATASETS_STORE_RECORDS_URL, headers=headers, verify=False)\n            json_data = response.json()\n            return json_data\n    \n    #Update the run status to Finished in the custom monitor instance\n    def update_monitor_instance(base_url, access_token, custom_monitor_instance_id, payload):\n        monitor_instance_url = base_url + '/v2/monitor_instances/' + custom_monitor_instance_id + '?update_metadata_only=true'\n        \n        patch_payload  = [\n            {\n                \"op\": \"replace\",\n                \"path\": \"/parameters\",\n                \"value\": payload\n            }\n        ]\n        headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n        response = requests.patch(monitor_instance_url, headers=headers, json = patch_payload, verify=False)\n        monitor_response = response.json()\n        return response.status_code, monitor_response\n    \n    #Add your code to compute the custom metrics. \n    def get_metrics(access_token, data_mart_id, subscription_id):\n        #Add the logic here to compute the metrics. Use the below metric names while creating the custom monitor definition\n        feedback_dataset_id = get_feedback_dataset_id(access_token, data_mart_id, subscription_id)\n        json_data = get_feedback_data(access_token, data_mart_id, feedback_dataset_id)\n        gender_less40_fav_prediction_ratio = 0\n        if json_data is not None:\n            fields = json_data['records'][0]['fields']\n            values = json_data['records'][0]['values']\n            import pandas as pd\n            feedback_data = pd.DataFrame(values, columns = fields)\n            female_less40_fav_prediction = len(feedback_data.query('Sex == \\'female\\' & Age <= 40 & Risk == \\'No Risk\\''))\n            male_less40_fav_prediction = len(feedback_data.query('Sex == \\'male\\' & Age <= 40 & Risk == \\'No Risk\\''))\n            gender_less40_fav_prediction_ratio = female_less40_fav_prediction / male_less40_fav_prediction\n\n        metrics = {\"specificity\": 1.2, \"sensitivity\": 0.85, \"gender_less40_fav_prediction_ratio\": gender_less40_fav_prediction_ratio, \"region\": \"us-south\"}\n        \n        return metrics\n        \n        \n    # Publishes the Custom Metrics to OpenScale\n    def publish_metrics(base_url, access_token, data_mart_id, subscription_id, custom_monitor_id, custom_monitor_instance_id, custom_monitoring_run_id, timestamp):\n        # Generate an monitoring run id, where the publishing happens against this run id\n        custom_metrics = get_metrics(access_token, data_mart_id, subscription_id)\n        measurements_payload = [\n                  {\n                    \"timestamp\": timestamp,\n                    \"run_id\": custom_monitoring_run_id,\n                    \"metrics\": [custom_metrics]\n                  }\n                ]\n        headers[\"Authorization\"] = \"Bearer {}\".format(access_token)\n        measurements_url = base_url + '/v2/monitor_instances/' + custom_monitor_instance_id + '/measurements'\n        response = requests.post(measurements_url, headers=headers, json = measurements_payload, verify=False)\n        published_measurement = response.json()\n     \n        return response.status_code, published_measurement\n        \n    \n    def publish( input_data ):\n        import datetime\n        timestamp = datetime.datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n        \n        payload = input_data.get(\"input_data\")[0].get(\"values\")\n        data_mart_id = payload['data_mart_id']\n        subscription_id = payload['subscription_id']\n        custom_monitor_id = payload['custom_monitor_id']\n        custom_monitor_instance_id = payload['custom_monitor_instance_id']\n        custom_monitor_instance_params  = payload['custom_monitor_instance_params']\n\n        base_url = parms['url'] + '/openscale' + '/' + data_mart_id\n        access_token = get_access_token()\n        \n        published_measurements = []\n        error_msg = None\n        \n        custom_monitoring_run_id = custom_monitor_instance_params[\"run_details\"][\"run_id\"]\n        try:\n            status_code, published_measurement = publish_metrics(base_url, access_token, data_mart_id, subscription_id, custom_monitor_id, custom_monitor_instance_id, custom_monitoring_run_id, timestamp)\n            if int(status_code) in [200, 201, 202]:\n                custom_monitor_instance_params[\"run_details\"][\"run_status\"] = \"finished\"\n                published_measurements.append(published_measurement)\n            else:\n                custom_monitor_instance_params[\"run_details\"][\"run_status\"] = \"error\"\n                custom_monitor_instance_params[\"run_details\"][\"run_error_msg\"] = published_measurement\n                error_msg = published_measurement\n                    \n            custom_monitor_instance_params[\"last_run_time\"] = timestamp\n            status_code, response = update_monitor_instance(base_url, access_token, custom_monitor_instance_id, custom_monitor_instance_params)\n            if not int(status_code) in [200, 201, 202]:\n                error_msg = response\n                \n        except Exception as ex:\n            error_msg = str(ex)\n        if error_msg is None:\n            response_payload = {\n                \"predictions\" : [{ \n                    \"values\" : published_measurements\n                }]\n\n            }\n        else:\n            response_payload = {\n                \"error_msg\": error_msg\n            }\n        \n        return response_payload\n        \n    return publish\n    ", "execution_count": null, "outputs": []}, {"metadata": {"id": "47683fe263944fafb25f856214b1ec70"}, "cell_type": "markdown", "source": "## 3. Register the custom metrics provider and create a deployment. <a name=\"deployment\"></a>"}, {"metadata": {"id": "d8948f9b03e849d9aba1f74d7e7830c3"}, "cell_type": "code", "source": "import json\nfrom ibm_watson_machine_learning import APIClient\n\nwml_client = APIClient(WML_CREDENTIALS)\nwml_client.version", "execution_count": null, "outputs": []}, {"metadata": {"id": "15601ba7120748b9902f57ca107067b6"}, "cell_type": "code", "source": "wml_client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {"id": "74c4db6b630d424a895daadee635b4c4"}, "cell_type": "code", "source": "#space_id = \"<Space Id>\" #update your space id\nspace_id = \"c5d28564-e451-4b3b-a3a7-d45bb6c9ef83\"\nwml_client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "06bb0042-e9b4-432a-9ada-8deb8c7f64dc"}, "cell_type": "markdown", "source": "### Remove existing function and deployment."}, {"metadata": {"id": "24d235e3-a887-4d8e-a3b6-7c2b5c6f32fc"}, "cell_type": "code", "source": "deployments_list = wml_client.deployments.get_details()\nfor deployment in deployments_list[\"resources\"]:\n    model_id = deployment[\"entity\"][\"asset\"][\"id\"]\n    deployment_id = deployment[\"metadata\"][\"id\"]\n    if deployment[\"metadata\"][\"name\"] == DEPLOYMENT_NAME:\n        print(\"Deleting deployment id\", deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print(\"Deleting model id\", model_id)\n        wml_client.repository.delete(model_id)\n\nwml_client.repository.list_functions()", "execution_count": null, "outputs": []}, {"metadata": {"id": "36a1b7cb5d3f48049fd42edcb471d157"}, "cell_type": "markdown", "source": "### Create the function meta properties.\n"}, {"metadata": {"id": "bd7e0a1e303c41b9b90815e8c2b33894"}, "cell_type": "code", "source": "software_spec_id =  wml_client.software_specifications.get_id_by_name('default_py3.7_opence')\nprint(software_spec_id)\nfunction_meta_props = {\n     wml_client.repository.FunctionMetaNames.NAME: PYTHON_FUNCTION_NAME,\n     wml_client.repository.FunctionMetaNames.SOFTWARE_SPEC_ID: software_spec_id\n     }", "execution_count": null, "outputs": []}, {"metadata": {"id": "e7fbbccd0f2344d097433ebdaf04469c"}, "cell_type": "markdown", "source": "### Store the Python function."}, {"metadata": {"id": "2d2aef0d0e1e47048cbf4abd94aac631"}, "cell_type": "code", "source": "function_artifact = wml_client.repository.store_function(meta_props=function_meta_props, function=custom_metrics_provider)\nfunction_uid = wml_client.repository.get_function_id(function_artifact)\nprint(\"Function UID = \" + function_uid)", "execution_count": null, "outputs": []}, {"metadata": {"id": "82a7d7df8fd344158108d57be229c5fa"}, "cell_type": "code", "source": "function_details = wml_client.repository.get_details(function_uid)\nfrom pprint import pprint\npprint(function_details)", "execution_count": null, "outputs": []}, {"metadata": {"id": "8b89b9d948f242ffa77bde26a8e9cb81"}, "cell_type": "markdown", "source": "### Deploy the Python function.\n"}, {"metadata": {"id": "d1c33df40ea94f4ca7f8de257818b417"}, "cell_type": "code", "source": "hardware_spec_id = wml_client.hardware_specifications.get_id_by_name('M')\nhardware_spec_id", "execution_count": null, "outputs": []}, {"metadata": {"id": "6774445edd764e708ba6b4612edaed71"}, "cell_type": "markdown", "source": "### Create deployment metadata for the Python function."}, {"metadata": {"id": "888a3c84fc06461f8ebc2120dfd58b22"}, "cell_type": "code", "source": "deploy_meta = {\n wml_client.deployments.ConfigurationMetaNames.NAME: DEPLOYMENT_NAME,\n wml_client.deployments.ConfigurationMetaNames.ONLINE: {},\n wml_client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { \"id\": hardware_spec_id}\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "3273246ba52a49a1a7e7e98964d2cb63"}, "cell_type": "markdown", "source": "### Create a deployment."}, {"metadata": {"id": "7b5a9733edfb484e9a8533d22581bd8d"}, "cell_type": "code", "source": "deployment_details = wml_client.deployments.create(function_uid, meta_props=deploy_meta)", "execution_count": null, "outputs": []}, {"metadata": {"id": "e97ac943-3e1f-40b1-80ab-4c0ba2f823b0"}, "cell_type": "markdown", "source": "### Get the scoring URL.\n"}, {"metadata": {"id": "bb56417d0e6e444484c25da9e6048aa5"}, "cell_type": "code", "source": "created_at = deployment_details['metadata']['created_at']\nfind_string_pos = created_at.find(\"T\")\nif find_string_pos is not -1:\n    current_date = created_at[0:find_string_pos]\nscoring_url = wml_client.deployments.get_scoring_href(deployment_details)\nscoring_url = scoring_url + \"?version=\"+current_date\nprint(scoring_url)\n", "execution_count": null, "outputs": []}, {"metadata": {"id": "73c85183a81f4997837587b2c64753c6"}, "cell_type": "markdown", "source": "## 4. Configure OpenScale. <a name=\"config\"></a>\n\nImport the required libraries and set up the Watson OpenScale Python client."}, {"metadata": {"id": "addc28e79d494e68a5cb6a942d4f8bac"}, "cell_type": "code", "source": "from ibm_watson_openscale import APIClient\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMeasurementRequest\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorMetricRequest\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import MetricThreshold\nfrom ibm_watson_openscale.supporting_classes.enums import MetricThresholdTypes\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import MonitorTagRequest\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import Target\nfrom ibm_watson_openscale.supporting_classes.enums import TargetTypes\nfrom ibm_watson_openscale.base_classes.watson_open_scale_v2 import IntegratedSystems\n\nfrom datetime import datetime, timezone, timedelta\nimport uuid", "execution_count": null, "outputs": []}, {"metadata": {"id": "1bb826627adf4a2ba2e2bc20d5ab25a2"}, "cell_type": "code", "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator,BearerTokenAuthenticator\n\nfrom ibm_watson_openscale import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\n\nauthenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\n#authenticator = BearerTokenAuthenticator(bearer_token=IAM_TOKEN) ## uncomment this line if using IAM token to authenticate\nwos_client = APIClient(authenticator=authenticator)\nwos_client.version", "execution_count": null, "outputs": []}, {"metadata": {"id": "09b669c1459a4f75ad0d040d433795e4"}, "cell_type": "markdown", "source": "## 5. Create the integrated system for the custom metrics provider. <a name=\"custom\"></a>\n\n\nUpdate the custom metrics deployment URL, which is created during the Python function creation in the integrated system. Watson OpenScale invokes the deployment URL at runtime to compute the custom metrics. \n\nYou must define the authentication type based on the communication with custom metrics deployment. Watson OpenScale supports 2 types of authentication: basic and bearer. If custom metrics deployment accepts the `basic` authentication type, then provide `auth_type=basic` otherwise use `auth_type=bearer`."}, {"metadata": {"id": "f119dff4-b24f-4a12-9843-0065729cc084"}, "cell_type": "code", "source": "auth_type = \"bearer\" #Supported values are basic and bearer\n\nif auth_type == \"basic\":\n    CUSTOM_METRICS_PROVIDER_CREDENTIALS = {\n        \"auth_type\":\"basic\",\n        \"username\":  \"*****\",# update the username here \n        \"password\": \"*****\"# Update the password here\n   }\n    \nif auth_type == \"bearer\":\n    CUSTOM_METRICS_PROVIDER_CREDENTIALS = {\n        \"auth_type\":\"bearer\",\n        \"token_info\": {\n           \"url\":  IAM_URL,\n           \"headers\": { \"Content-type\": \"application/x-www-form-urlencoded\" }, # update the headers here \n           \"payload\": \"grant_type=urn:ibm:params:oauth:grant-type:apikey&response_type=cloud_iam&apikey=\"+CLOUD_API_KEY, # update the payload here \n           \"method\": \"POST\" # # update the http method here \n        }        \n    }\n    \n  #if custom metrics deployment is on other cpd cluster or some other cloud then please uncomment and update \n  #the below \"TOKEN_INFO\" properties to generate the token to communicate to the custom metrics deployment url\n  #Here are the sample values given in the token_info\n    #TOKEN_INFO = {\n    #    \"url\":  \"https://iam.ng.bluemix.net/oidc/token\", # update the token generation here \n    #    \"headers\": { \"Content-type\": \"application/x-www-form-urlencoded\" }, # update the headers here \n    #    \"payload\": \"grant_type=urn:ibm:params:oauth:grant-type:apikey&response_type=cloud_iam&apikey=<api_key>\", # update the payload here \n    #    \"method\": \"POST\" # # update the http method here \n    #}\n    #CUSTOM_METRICS_PROVIDER_CREDENTIALS[\"token_info\"] = TOKEN_INFO\n", "execution_count": null, "outputs": []}, {"metadata": {"id": "296fc469-b17b-412d-a252-f392b00e05aa"}, "cell_type": "markdown", "source": "### Remove existing integrated system "}, {"metadata": {"id": "198929f5-1ad4-4e9a-ab35-7f180ebeb1ed"}, "cell_type": "code", "source": "# Delete existing custom metrics provider integrated systems if present\nintegrated_systems = IntegratedSystems(wos_client).list().result.integrated_systems\nfor system in integrated_systems:\n    if system.entity.type == 'custom_metrics_provider' and system.entity.name == CUSTOM_METRICS_PROVIDER_NAME:\n        print(\"Deleting integrated system {}\".format(system.entity.name))\n        IntegratedSystems(wos_client).delete(integrated_system_id=system.metadata.id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "31eb1a73-780b-4565-97aa-53a2ad1e6d6c"}, "cell_type": "code", "source": "custom_metrics_integrated_system = IntegratedSystems(wos_client).add(\n    name=CUSTOM_METRICS_PROVIDER_NAME,\n    description=CUSTOM_METRICS_PROVIDER_NAME,\n    type=\"custom_metrics_provider\",\n    credentials= CUSTOM_METRICS_PROVIDER_CREDENTIALS,\n    connection={\n        \"display_name\": CUSTOM_METRICS_PROVIDER_NAME,\n        \"endpoint\": scoring_url\n    }\n).result\n\nintegrated_system_id = custom_metrics_integrated_system.metadata.id\nprint(custom_metrics_integrated_system)", "execution_count": null, "outputs": []}, {"metadata": {"id": "8bb4bd32-b0b5-4ddc-835f-c40cab1c779a"}, "cell_type": "markdown", "source": "## 6. Set up the custom monitor definition and instance. <a name=\"instance\"></a>\n"}, {"metadata": {"id": "59b9827e31ab45edbad79789213703d1"}, "cell_type": "markdown", "source": "### Check for the existence of the custom monitor definition.\n"}, {"metadata": {"id": "14e3f1e2e2f24bfd86f86feee24619e0"}, "cell_type": "code", "source": "def get_custom_monitor_definition():\n    monitor_definitions = wos_client.monitor_definitions.list().result.monitor_definitions\n    for definition in monitor_definitions:\n        if CUSTOM_MONITOR_NAME == definition.entity.name:\n            return definition\n    return None   ", "execution_count": null, "outputs": []}, {"metadata": {"id": "30720ce70ddf485782ebf767a606366b"}, "cell_type": "markdown", "source": "### Create the  custom monitor definition.\n\nUpdate the custom metric names, threshold types (`LOWER_LIMIT`, `UPPER_LIMIT`) and default values as required. You can define the threshold type as lower limit, upper limit, or both."}, {"metadata": {"id": "18f34443-76c8-4b9c-85da-a58b10003e7c"}, "cell_type": "code", "source": "###################################################################\n# Update your custom monitor metrics names in the following field. Use the same metric names for creating the \n# monitor definition and publishing the metrics to Openscale in your python function\n####################################################################\nCUSTOM_MONITOR_METRICS_NAMES = ['sensitivity','specificity', 'gender_less40_fav_prediction_ratio']\n#Update the tag values if you want to fetch the metrics by tags\nTAGS= ['region']\nTAG_DESCRIPTION =['customer geographical region'] ", "execution_count": null, "outputs": []}, {"metadata": {"id": "df7bfd7b6f274d54897133f94dc14310"}, "cell_type": "code", "source": "#Update the Threshold types and default values of the metrics\ndef custom_metric_definitions():\n    \n    metrics = [MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[0],\n                                    thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.8)]),\n              MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[1],\n                                 thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.6),MetricThreshold(type=MetricThresholdTypes.UPPER_LIMIT, default=1)]),\n              MonitorMetricRequest(name=CUSTOM_MONITOR_METRICS_NAMES[2],\n                                 thresholds=[MetricThreshold(type=MetricThresholdTypes.LOWER_LIMIT, default=0.6),MetricThreshold(type=MetricThresholdTypes.UPPER_LIMIT, default=1)])]\n    #Comment the below tags code if there are no tags to be created\n    tags = [MonitorTagRequest(name=TAGS[0], description=TAG_DESCRIPTION[0])]\n    \n    return metrics, tags", "execution_count": null, "outputs": []}, {"metadata": {"id": "79e2f275e13948e5834e77ec4dfadeb6"}, "cell_type": "code", "source": "def create_custom_monitor_definition():\n    # check if the custom monitor definition already exists or not\n    existing_definition = get_custom_monitor_definition()\n\n    # if it does not exists, then create a new one.\n    if existing_definition is None:\n        metrics, tags = custom_metric_definitions()\n        custom_monitor_details = wos_client.monitor_definitions.add(name=CUSTOM_MONITOR_NAME, metrics=metrics, tags=tags, background_mode=False).result\n    else:\n        # otherwise, send the existing definition\n        custom_monitor_details = existing_definition\n    return custom_monitor_details", "execution_count": null, "outputs": []}, {"metadata": {"id": "8d352bcf09e646d0a6d174e54078f2c6"}, "cell_type": "code", "source": "custom_monitor_details = create_custom_monitor_definition()\ncustom_monitor_id = custom_monitor_details.metadata.id\ncustom_monitor_id", "execution_count": null, "outputs": []}, {"metadata": {"id": "3a0dbf50822c40c680cc9430eb88306c"}, "cell_type": "markdown", "source": "### Check the existence of custom monitor instance.\n"}, {"metadata": {"id": "7176b942f61f4d0381fb63b92101e76f"}, "cell_type": "code", "source": "def get_custom_monitor_instance(custom_monitor_id):\n    monitor_instances = wos_client.monitor_instances.list(data_mart_id = WOS_GUID, monitor_definition_id = custom_monitor_id, target_target_id = subscription_id).result.monitor_instances\n    if len(monitor_instances) == 1:\n        return monitor_instances[0]\n    return None", "execution_count": null, "outputs": []}, {"metadata": {"id": "32c84dce-c23a-42ef-b874-477a71b4ef36"}, "cell_type": "code", "source": "# Openscale MRM service invokes custom metrics deployment url during runtime and wait for the default time of 60 second's to \n# to check the run status ie finished/Failed and fetch the latest measurement. Increase the wait time, if the runtime deployment \n# takes more than 60 seconds to compute and publish the custom metrics \n\n#Update the wait time here.\ncustom_metrics_wait_time = 60 #time in seconds <update the time here>", "execution_count": null, "outputs": []}, {"metadata": {"id": "5c49fe8f-7921-410c-9983-6f998522d715"}, "cell_type": "markdown", "source": "### Update the custom monitor instance."}, {"metadata": {"id": "67ed74d7-928a-4f64-a7f7-dc9bfb5d5fbc"}, "cell_type": "code", "source": "def update_custom_monitor_instance(custom_monitor_instance_id):\n    payload = [\n     {\n       \"op\": \"replace\",\n       \"path\": \"/parameters\",\n       \"value\": {\n           \"custom_metrics_provider_id\": integrated_system_id,\n           \"custom_metrics_wait_time\":   custom_metrics_wait_time \n       }\n     }\n    ]\n    response = wos_client.monitor_instances.update(custom_monitor_instance_id, payload, update_metadata_only = True)\n    result = response.result\n    return result", "execution_count": null, "outputs": []}, {"metadata": {"id": "678df1de54164d6284cdf9f255affd8a"}, "cell_type": "markdown", "source": "### For the custom monitor definition, create a custom monitor instance.\n"}, {"metadata": {"id": "97624e1c0a8e4648af05162690c96c66"}, "cell_type": "code", "source": "def create_custom_monitor_instance(custom_monitor_id):\n    # Check if an custom monitor instance already exists\n    existing_monitor_instance = get_custom_monitor_instance(custom_monitor_id)\n\n    # If it does not exist, then create one\n    if existing_monitor_instance is None:\n        target = Target(\n                target_type=TargetTypes.SUBSCRIPTION,\n                target_id=subscription_id\n            )\n        parameters = {\n            \"custom_metrics_provider_id\": integrated_system_id,\n            \"custom_metrics_wait_time\":   custom_metrics_wait_time \n        }\n        # create the custom monitor instance id here.\n        custom_monitor_instance_details = wos_client.monitor_instances.create(\n                    data_mart_id=WOS_GUID,\n                    background_mode=False,\n                    monitor_definition_id=custom_monitor_id,\n                    target=target,\n                    parameters=parameters\n        ).result\n    else:\n        # otherwise, update the existing one with latest integrated system details.\n        instance_id = existing_monitor_instance.metadata.id\n        custom_monitor_instance_details = update_custom_monitor_instance(instance_id)\n    return custom_monitor_instance_details", "execution_count": null, "outputs": []}, {"metadata": {"id": "ffb159c5c1f54ffe9f1b7a48b727810c"}, "cell_type": "code", "source": "monitor_instance_details = create_custom_monitor_instance(custom_monitor_id)\ncustom_monitor_instance_id = monitor_instance_details.metadata.id\nprint(monitor_instance_details)", "execution_count": null, "outputs": []}, {"metadata": {"id": "1c0fca1109bf4c048fe61d6de5815179"}, "cell_type": "markdown", "source": "## Recap of the steps performed in this notebook\n\n- Create a python function\n- Deploy the python function to WML\n- Create an OpenScale Integrated System pointing to the python function\n- Create a Custom Monitor Definition mentioning various custom metrics\n- Create a Custom Monitor Instance and specify the Integrated System ID in the monitor instance configuration."}, {"metadata": {"id": "8f200d01939d4ce888ec403ac4f59725"}, "cell_type": "markdown", "source": "### Upon publishing required payload logging or feedback logging data, please visit OpenScale console and run `Evaluate Now` from OpenScale Model Risk Management dashboard / Actions menu to evaluate the configured Custom Metrics Provider. For a production based subscriptions, the custom metrics provider would be invoked periodically."}, {"metadata": {"id": "6e66248c-bbdb-4644-9dd8-b871e9aafaca"}, "cell_type": "markdown", "source": "# [OPTIONAL STEP] Invoke the custom metrics deployment Python function as part of this notebook.\n\nValidate the custom metrics provider deployment by providing the correct set of paramaters to generate the custom metrics."}, {"metadata": {"id": "8fd2f8ec-bab9-4f9e-b3be-22822155fd9b"}, "cell_type": "code", "source": "import uuid\nparameters = {\n    \"custom_metrics_provider_id\": integrated_system_id,\n    \"custom_metrics_wait_time\":   custom_metrics_wait_time,\n    \"run_details\": {\n    \"run_id\": str(uuid.uuid4()),\n    \"run_status\": \"Running\"\n    }\n}\n\npayload= {\n    \"data_mart_id\" : WOS_GUID,\n    \"subscription_id\" : subscription_id,\n    \"custom_monitor_id\" : custom_monitor_id,\n    \"custom_monitor_instance_id\" : custom_monitor_instance_id,\n    \"custom_monitor_instance_params\": parameters\n    \n}\n\ninput_data= { \"input_data\": [ { \"values\": payload } ]\n            }\n\n\ndeployment_uid=wml_client.deployments.get_uid(deployment_details)\njob_details = wml_client.deployments.score(deployment_uid, input_data)\npprint(job_details)", "execution_count": null, "outputs": []}, {"metadata": {"id": "210c3b5835494f67812f0704bad84705"}, "cell_type": "markdown", "source": "## Congratulations\n\nYou have finished configuring Custom Monitor Definition and Monitor instance for your Subscription. You can now run the custom monitor from `Watson OpenScale Dashboard`(http://aiopenscale.cloud.ibm.com). Click the tile of your model and select `Evaluate Now` option from `Actions` drop down menu to run the monitor."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 1}